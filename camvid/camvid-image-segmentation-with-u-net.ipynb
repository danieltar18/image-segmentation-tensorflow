{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CamVid (Cambridge-Driving Labeled Video Database)\n\n## Description\nThe Cambridge-driving Labeled Video Database (CamVid) provides ground truth labels that associate each pixel with one of 32 semantic classes. This dataset is often used in (real-time) semantic segmentation research.\n\nThe dataset is split up as follows:\n\n* 367 training pairs\n* 101 validation pairs\n* 233 test pairs\n\nThe images and masks for each split are in a separate directory.\n\nCitations:\n\n[1] Brostow, Shotton, Fauqueur, Cipolla. **Segmentation and Recognition Using Structure from Motion Point Clouds**, \n_European Conference on Computer Vision (ECCV)_, 2008.\n\n[2] Brostow, Fauqueur, Cipolla. **Semantic Object Classes in Video: A High-Definition Ground Truth Database**, \n_Pattern Recognition Letters_.\n\nThe original dataset can be found here:\nhttp://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid\n\nSource / Contact:\nhttp://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid\n\nKaggle Source:\nhttps://www.kaggle.com/datasets/carlolepelaars/camvid","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport tarfile \nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:09.717634Z","iopub.execute_input":"2025-02-03T18:35:09.717884Z","iopub.status.idle":"2025-02-03T18:35:09.732827Z","shell.execute_reply.started":"2025-02-03T18:35:09.717863Z","shell.execute_reply":"2025-02-03T18:35:09.731952Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Initialize notebook constants/config class\nclass CFG():\n    RANDOM_SEED=42\n    IMG_HEIGHT=256\n    IMG_WIDTH=256\n    BATCH_SIZE=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:09.733891Z","iopub.execute_input":"2025-02-03T18:35:09.734238Z","iopub.status.idle":"2025-02-03T18:35:09.751474Z","shell.execute_reply.started":"2025-02-03T18:35:09.734206Z","shell.execute_reply":"2025-02-03T18:35:09.750612Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"files_path = \"../input/camvid/CamVid/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:09.752213Z","iopub.execute_input":"2025-02-03T18:35:09.752544Z","iopub.status.idle":"2025-02-03T18:35:09.771556Z","shell.execute_reply.started":"2025-02-03T18:35:09.752493Z","shell.execute_reply":"2025-02-03T18:35:09.770378Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"unique_classes = pd.read_csv(files_path + \"class_dict.csv\")\nunique_classes.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:09.776155Z","iopub.execute_input":"2025-02-03T18:35:09.776402Z","iopub.status.idle":"2025-02-03T18:35:09.804404Z","shell.execute_reply.started":"2025-02-03T18:35:09.776371Z","shell.execute_reply":"2025-02-03T18:35:09.803465Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        name    r    g    b\n0     Animal   64  128   64\n1    Archway  192    0  128\n2  Bicyclist    0  128  192\n3     Bridge    0  128   64\n4   Building  128    0    0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>r</th>\n      <th>g</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Animal</td>\n      <td>64</td>\n      <td>128</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Archway</td>\n      <td>192</td>\n      <td>0</td>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bicyclist</td>\n      <td>0</td>\n      <td>128</td>\n      <td>192</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bridge</td>\n      <td>0</td>\n      <td>128</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Building</td>\n      <td>128</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"color_map = unique_classes.drop(\"name\", axis=1)\ncolor_map_tf = tf.constant(color_map, dtype=tf.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:09.805598Z","iopub.execute_input":"2025-02-03T18:35:09.805915Z","iopub.status.idle":"2025-02-03T18:35:10.032071Z","shell.execute_reply.started":"2025-02-03T18:35:09.805887Z","shell.execute_reply":"2025-02-03T18:35:10.031129Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_files = tf.data.Dataset.list_files(files_path + \"train/*.png\") \nval_files = tf.data.Dataset.list_files(files_path + \"val/*.png\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:10.033082Z","iopub.execute_input":"2025-02-03T18:35:10.033351Z","iopub.status.idle":"2025-02-03T18:35:10.603623Z","shell.execute_reply.started":"2025-02-03T18:35:10.033327Z","shell.execute_reply":"2025-02-03T18:35:10.602813Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def rgb_class_convert(label_rgb, color_map, inverse=False):\n\n    if not inverse:\n        label_expanded = tf.expand_dims(label_rgb, axis=-2)  # [H, W, 1, 3]\n        color_map_expanded = tf.reshape(color_map, [1, 1, -1, 3])  # [1, 1, N, 3]\n        matches = tf.reduce_all(tf.equal(label_expanded, color_map_expanded), axis=-1)  # [H, W, num_classes]\n        class_indices = tf.argmax(tf.cast(matches, tf.int32), axis=-1)\n    else:\n        label_indices = tf.argmax(label_rgb, axis=-1)\n        rgb_image = tf.gather(color_map, label_indices)\n        class_indices = rgb_image\n\n    return class_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:10.604408Z","iopub.execute_input":"2025-02-03T18:35:10.604672Z","iopub.status.idle":"2025-02-03T18:35:10.609958Z","shell.execute_reply.started":"2025-02-03T18:35:10.604651Z","shell.execute_reply":"2025-02-03T18:35:10.609028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"@tf.function\ndef load_image(img_path: tf.Tensor, ds_type: str):\n    \n    image_bytes = tf.io.read_file(img_path)\n    image = tf.image.decode_png(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, (CFG.IMG_HEIGHT, CFG.IMG_WIDTH))\n    image_file = tf.strings.split(tf.strings.split(img_path, os.sep)[-1], \".\")[0]\n\n    if ds_type == \"train\":\n        label_path = tf.strings.join([files_path, \"train_labels/\", image_file, \"_L.png\"])\n    elif ds_type == \"val\":\n        label_path = tf.strings.join([files_path, \"val_labels/\", image_file, \"_L.png\"])\n    else:\n        label_path = tf.strings.join([files_path, \"test_labels/\", image_file, \"_L.png\"])\n\n    label = tf.io.read_file(label_path)\n    label = tf.image.decode_png(label, channels=3)\n    label = tf.image.resize(label, (CFG.IMG_HEIGHT, CFG.IMG_WIDTH), method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    label = tf.cast(label, tf.int32)\n    label = rgb_class_convert(label, color_map_tf)\n    label = tf.one_hot(label, depth=len(unique_classes))\n    label = tf.cast(label, tf.float32)\n        \n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:10.611722Z","iopub.execute_input":"2025-02-03T18:35:10.611956Z","iopub.status.idle":"2025-02-03T18:35:10.626277Z","shell.execute_reply.started":"2025-02-03T18:35:10.611937Z","shell.execute_reply":"2025-02-03T18:35:10.625526Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def configure_performance(ds, is_train_ds: bool):\n    if is_train_ds:\n        ds = ds.cache()\n        ds = ds.shuffle(100)\n        ds = ds.batch(CFG.BATCH_SIZE)\n        ds = ds.prefetch(tf.data.AUTOTUNE)\n    else:            \n        ds = ds.cache()\n        ds = ds.batch(CFG.BATCH_SIZE)\n        ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:10.627085Z","iopub.execute_input":"2025-02-03T18:35:10.627350Z","iopub.status.idle":"2025-02-03T18:35:10.643288Z","shell.execute_reply.started":"2025-02-03T18:35:10.627330Z","shell.execute_reply":"2025-02-03T18:35:10.642452Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def display_imgs(display_list, unique_classes):\n    \n    display_list = list(display_list)\n    display_list.insert(1, rgb_class_convert(display_list[1], color_map_tf, inverse=True)) # Inverse Transformed to color scale\n    display_list[-1] = tf.expand_dims(tf.argmax(display_list[-1], axis=-1), axis=-1)\n    \n    num_images = len(display_list)\n    fig, axes = plt.subplots(1, num_images, figsize=(14, 10))\n    \n    titles = ['Input Image', 'True Mask', 'Class-indexed Mask', 'Predicted Mask']\n    \n    for ax, image, title in zip(axes, display_list, titles):\n        ax.set_title(title)\n        ax.imshow(tf.keras.utils.array_to_img(image))\n        ax.axis('off')\n    \n    mask_tensor = display_list[2]\n    mask_np = mask_tensor.numpy()\n    unique_ids = np.unique(mask_np)\n    unique_classes_filtered = unique_classes.iloc[unique_ids]\n    \n    handles = []\n    for _, row in unique_classes_filtered.iterrows():\n        label = f\"{row['name']}\"\n        color_float = (row['r'] / 255.0, row['g'] / 255.0, row['b'] / 255.0)\n        patch = mpatches.Patch(color=color_float, label=label)\n        handles.append(patch)\n    \n    max_cols = 5\n    ncols = min(len(handles), max_cols)   \n    fig.legend(handles=handles,\n               title=\"True Mask Classes Colors (only)\",\n               loc='lower center',\n               bbox_to_anchor=(0.5, 0.2),\n               ncol=ncols,\n               borderaxespad=0.)\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:11.464475Z","iopub.execute_input":"2025-02-03T18:35:11.464819Z","iopub.status.idle":"2025-02-03T18:35:11.471727Z","shell.execute_reply.started":"2025-02-03T18:35:11.464788Z","shell.execute_reply":"2025-02-03T18:35:11.470877Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_ds = train_files.map(lambda img_path: load_image(img_path, ds_type=\"train\"), num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval_ds = val_files.map(lambda img_path: load_image(img_path, ds_type=\"val\"), num_parallel_calls=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:12.442035Z","iopub.execute_input":"2025-02-03T18:35:12.442329Z","iopub.status.idle":"2025-02-03T18:35:12.848104Z","shell.execute_reply.started":"2025-02-03T18:35:12.442308Z","shell.execute_reply":"2025-02-03T18:35:12.847420Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"sample_img, sample_label = next(iter(train_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:12.849138Z","iopub.execute_input":"2025-02-03T18:35:12.849346Z","iopub.status.idle":"2025-02-03T18:35:13.126897Z","shell.execute_reply.started":"2025-02-03T18:35:12.849329Z","shell.execute_reply":"2025-02-03T18:35:13.126008Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"display_imgs([sample_img, sample_label], unique_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:13.829174Z","iopub.execute_input":"2025-02-03T18:35:13.829460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = configure_performance(train_ds, is_train_ds=True)\nval_ds = configure_performance(val_ds, is_train_ds=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T18:35:25.917323Z","iopub.execute_input":"2025-02-03T18:35:25.917687Z","iopub.status.idle":"2025-02-03T18:35:25.933947Z","shell.execute_reply.started":"2025-02-03T18:35:25.917657Z","shell.execute_reply":"2025-02-03T18:35:25.933176Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define U-Net with MobileNetV2 Pretrained Model","metadata":{}},{"cell_type":"code","source":"# References for Conv2DTranspose and VGG16 model\n# (1): https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2\n# (2): https://www.tensorflow.org/tutorials/images/segmentation\n# (3): https://keras.io/api/applications/mobilenet/\n\ndef unet_mobilenetv2(input_shape=(256, 256, 3), num_classes=None):\n\n    base_model = tf.keras.applications.MobileNetV2(\n        input_shape=(224, 224),\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    layer_names = [\n        'block_1_expand_relu',   \n        'block_3_expand_relu', \n        'block_6_expand_relu',\n        'block_13_expand_relu', \n        'block_16_project' \n    ]\n    \n    layers_outputs = [base_model.get_layer(name).output for name in layer_names]\n    encoder = tf.keras.Model(inputs=base_model.input, outputs=layers_outputs)\n    encoder.trainable = False\n    \n    inputs = tf.keras.Input(shape=input_shape)\n    skips = encoder(inputs)\n    \n    x = skips[-1]\n    decoder_filters = [512, 256, 128, 64]\n    \n    for i in range(1, len(skips)):\n        x = tf.keras.layers.Conv2DTranspose(decoder_filters[i-1], kernel_size=3, strides=2, padding='same', activation='relu')(x)\n        skip_connection = skips[-(i+1)]\n        x = tf.keras.layers.Concatenate()([x, skip_connection])\n        x = tf.keras.layers.Conv2D(decoder_filters[i-1], kernel_size=3, padding='same', activation='relu')(x)\n        x = tf.keras.layers.Conv2D(decoder_filters[i-1], kernel_size=3, padding='same', activation='relu')(x)\n    \n    x = tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n    x = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)\n    x = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(x)\n    \n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size=1, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build and display the model summary.\nmodel = unet_mobilenetv2(input_shape=(256, 256, 3), num_classes=len(unique_classes))\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2])\n    union = tf.reduce_sum(y_true, axis=[1,2]) + tf.reduce_sum(y_pred, axis=[1,2])\n    dice = (2 * intersection + smooth) / (union + smooth) # multiply by two because of union divider\n    \n    return 1 - tf.reduce_mean(dice) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def total_loss(y_true, y_pred):\n    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n    dice = dice_loss(y_true, y_pred)\n    return ce + dice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-5,\n    warmup_target=0.001,\n    warmup_steps=1605,\n    decay_steps=14495,\n    alpha=0.0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n    loss=total_loss,\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 10\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}